# word-vectors-and-embeddings
- Word embeddings involves transforming words into numerical representations to enable machines understand and process human language.
- Unlike one-hot encoding,they capture semantic meaning between words.
- eg:

"King" → [0.25, -0.1, 0.7, ...]

"Queen" → [0.24, -0.09, 0.69, ...]

- Practical implementation of CBOW AND N-Gram models for word embeddings
